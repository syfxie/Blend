{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8cf13e-5fc0-4fdb-b49a-ea1278b51d87",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a81fd-ff34-4b10-9057-bc15f9885b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293c09b-ff61-4c71-a195-0fb63fe5750a",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5560d1-30c0-4240-95eb-2f32a3049fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU can accomodate greater image sizes and faster image synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766c261-afc9-435d-86a9-9aa9663d9a6e",
   "metadata": {},
   "source": [
    "## Load and Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ba359-5716-40ba-9cfd-e7abc8ff443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./static\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2c994-804a-4c59-84f8-8c9498b462cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, size=(128, 128)): # using default size for CPU\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    loader = transforms.Compose([\n",
    "        transforms.Resize(size),  # resize\n",
    "        transforms.ToTensor()])  # transform image into tensor\n",
    "\n",
    "    image = loader(image).unsqueeze(0) # models in torch.nn requires inputs with a batch dimension\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "def show_image(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # clone the tensor to avoid modifying the original\n",
    "\n",
    "    # print(\"tensor to display: \", image)\n",
    "    \n",
    "    if image.dim() == 4:\n",
    "        image = image.squeeze(0) # remove artificial batch dimension\n",
    "    \n",
    "    image = transforms.ToPILImage()(image)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee22cd0-7751-4ef4-9ccd-5656036587b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128, 128)\n",
    "content_image = load_image(os.path.join(image_dir, \"content_image.jpeg\"), image_size)\n",
    "style_image = load_image(os.path.join(image_dir, \"style_reference.jpeg\"), image_size)\n",
    "\n",
    "print('content image shape: ', content_image.size())\n",
    "print('style image shape: ', style_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beff817-c2cf-42fa-9e99-236fa5710544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_image(content_image, \"Content Image\")\n",
    "show_image(style_image, \"Style Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da776cb7-c823-48f0-a8ff-0b55d63e1243",
   "metadata": {},
   "source": [
    "## Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d32d7a-c0a9-4970-aaa5-8d884a567d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Gram matrix is the result of multiplying a matrix by its transpose.\n",
    "# Since our inputs will by pytorch tensors, we first need to manipulate the shape of the tensor to become a 2D matrix\n",
    "# before performing matrix multiplication.\n",
    "\n",
    "# Since the style features of an image are in the higher layers of the network, \n",
    "# the resulting matrix must be normalized to reduce influence of the first layers during gradient descent.\n",
    "\n",
    "\n",
    "def gram_matrix(input):\n",
    "    '''\n",
    "    Computes the Gram matrix of a tensor.\n",
    "    '''\n",
    "    print(\"========calculating gram matrix========\")\n",
    "\n",
    "    print(\"original shape: \", input.size())\n",
    "\n",
    "    if input.dim() == 3:\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "    shape = input.size()\n",
    "    print(\"input shape: \", shape)\n",
    "\n",
    "    try:\n",
    "        reshaped = input.view(shape[0] * shape[1], shape[2] * shape[3])  # flatten tensor into 2D\n",
    "        print(\"flattened tensor shape: \", reshaped.size())\n",
    "        \n",
    "        # multiply matrix by its transpose to compute the gram product\n",
    "        result = torch.mm(reshaped, reshaped.t())\n",
    "\n",
    "        # normalize the matrix to scale each value within 0-1\n",
    "        result = result.div(shape[0] * shape[1] * shape[2] * shape[3])\n",
    "\n",
    "        print(\"Normalized matrix shape: \", result.size())\n",
    "        return result\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"Input tensor must have 4 dimensions. Received {}D tensor instead\".format(len(shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac37b7-9fb8-49b3-982b-860ab89dade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = gram_matrix(style_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb91234-8524-4225-99eb-2ff36a04a556",
   "metadata": {},
   "source": [
    "### Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797ffbe-4294-42ef-b547-6282e33f6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the style and content loss as modules to add them into the model\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target).detach()  # the target must be detached to compute the gradient since it needs to be a static value\n",
    "\n",
    "    def forward(self, input):  # defines the computation (forward propagation) the module performs on the input\n",
    "        # print('Style loss forward input, target: ', input, self.target)\n",
    "        \n",
    "        G = gram_matrix(input)\n",
    "        # print('gram matrix: ', G)\n",
    "        \n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "    # back propagation method is defined automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f2304-c4de-4c4f-a19d-0aa53147e638",
   "metadata": {},
   "source": [
    "### Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41c897-b005-4bcc-bfd2-fe11202d842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print('Content loss forward input, target: ', input, self.target)\n",
    "        \n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319b1e0-5c2d-42ac-abd9-6740788760d0",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec14031-fb92-48e0-9170-713d93646667",
   "metadata": {},
   "source": [
    "\"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\" (https://pytorch.org/vision/0.8/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7eb9b0-993b-4236-abfd-e3e657daa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# normalization module to normalize input images for the VGG-19 model\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        # super(Normalization, self).__init__()\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "        print(self.mean)\n",
    "        print(self.mean.size())\n",
    "\n",
    "        print(self.std)\n",
    "        print(self.std.size())\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input - self.mean) / self.std # normalization formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870c8b7-1abf-4e0b-be40-014777ae938e",
   "metadata": {},
   "source": [
    "## L-BFGS Optimizer\n",
    "\n",
    "We will use the L-BFGS optimizer to optimize the input image's features to minimize the style/content loss. Notice how this differs from the usual application of optimizing model params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0a9b8-1980-4987-872e-cec18805e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs_optimizer(input):\n",
    "    optimizer = torch.optim.LBFGS([input.requires_grad_()]) # records operations applied to the input image\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2620d-6864-48fc-9327-6217affb7dbb",
   "metadata": {},
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727786f1-7b50-47ff-8490-ae4189899438",
   "metadata": {},
   "source": [
    "The paper only uses the first module of the VGG-19 model (features), which contains the convolution and pooling layers to extract the content and style representations. The classifier module in not needed as we are not performing any image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f446ccf-77f6-43c8-a282-cf66ecd55628",
   "metadata": {},
   "source": [
    "Note: we will use the model in evaluation mode (.eval()) since it may behave different in evaluation vs. training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c077a97-c8c5-4957-883a-774b9ed52a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see summary of model\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee76100-6195-4e4d-9fc9-689571d962b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480d76d-3f1e-4b2a-8363-01738de3228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_network = models.vgg19(weights='DEFAULT').features.to(device).eval()\n",
    "summary(vgg_network, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f95ee5-1acd-4a52-b18f-e4df0f50ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers used in the paper to determine content/style representations\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "last_layer = 'conv_5'  # Losses stop being calculated after this layer\n",
    "\n",
    "def compute_losses(network, style_image, content_image, mean=mean, std=std, content_layers=content_layers, style_layers=style_layers):\n",
    "    print('content image shape: ', content_image.size())\n",
    "    print('style image: shape ', style_image.size())\n",
    "    \n",
    "    vgg_network = copy.deepcopy(network)\n",
    "    normalization = Normalize(mean, std).to(device)\n",
    "\n",
    "    # make a new sequential model with custom loss/normalization modules\n",
    "    nst_model = nn.Sequential(normalization)\n",
    "\n",
    "    style_losses = []\n",
    "    content_losses = []\n",
    "\n",
    "    idx = 0\n",
    "    for layer in vgg_network.children():\n",
    "\n",
    "        # check type of layer\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = 'conv_'\n",
    "            idx += 1\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_'\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        name += str(idx)\n",
    "        nst_model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = nst_model(content_image).detach()\n",
    "            print('target shape: ', target.size())\n",
    "            \n",
    "            content_loss = ContentLoss(target)\n",
    "            print('content loss: ', content_loss)\n",
    "            \n",
    "            nst_model.add_module(\"content_loss_{}\".format(idx), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target = nst_model(style_image).detach()\n",
    "            print('target shape: ', target.size())\n",
    "            \n",
    "            style_loss = StyleLoss(target)\n",
    "            print('style loss: ', style_loss)\n",
    "            \n",
    "            nst_model.add_module(\"style_loss_{}\".format(idx), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "        if name == last_layer:\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "    # # for i in range(len(new_model) - 1, -1, -1):\n",
    "    # for i in reversed(len(new_model) - 1):\n",
    "    #     if isinstance(new_model[i], ContentLoss) or isinstance(new_model[i], StyleLoss):\n",
    "    #         break  # we don't need the layers after the last content/style loss is computed\n",
    "\n",
    "    # new_model = new_model[:(i + 1)]\n",
    "\n",
    "    return nst_model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb0eb1-1635-4b54-bf48-433aec21b292",
   "metadata": {},
   "source": [
    "## Preparing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d0ffc-80ed-4b16-bc22-5c2f3cd2504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.randn((128, 128), device=device)  # we will use a 128x128 whitenoise image as input\n",
    "\n",
    "plt.figure()\n",
    "show_image(input_image, title='Input Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8f125-5823-4367-8d9c-e1a406496062",
   "metadata": {},
   "source": [
    "## Apply the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2dba5d-e838-4980-95b1-1009d6dc3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(network, input, content_image, style_image, mean=mean, std=std, \n",
    "                   steps=100, style_weight=20000, content_weight=100):\n",
    "\n",
    "    # get model, losses, and optimizer\n",
    "    model, style_losses, content_losses = compute_losses(network, style_image, content_image)\n",
    "    optimizer = lbfgs_optimizer(input)\n",
    "    \n",
    "    i = 0\n",
    "    while i <= steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_image.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_image)\n",
    "            \n",
    "            style_loss = 0\n",
    "            content_loss = 0\n",
    "\n",
    "            for loss in style_losses:\n",
    "                style_loss += loss.loss\n",
    "            for loss in content_losses:\n",
    "                content_loss += loss.loss\n",
    "\n",
    "            style_loss *= style_weight\n",
    "            content_loss *= content_weight\n",
    "\n",
    "            total_loss = style_loss + content_loss\n",
    "            print('total loss: ', total_loss)\n",
    "            \n",
    "            total_loss.backward()  # Compute the gradient\n",
    "            print('total loss after back propagation: ', total_loss)\n",
    "\n",
    "            # i += 1\n",
    "            # if i % 50 == 0:\n",
    "            #     print(\"{}/{} steps:\".format(i, steps))\n",
    "            #     print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "            #         style_loss.item(), content_loss.item()))\n",
    "            #     print()\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        i += 1\n",
    "\n",
    "    # a last correction...\n",
    "    input.data.clamp_(0, 1)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2bfaf-8b7b-49bf-86f1-b00b0d9cec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = style_transfer(vgg_network, input_image, content_image, style_image)\n",
    "\n",
    "plt.figure()\n",
    "imshow(result, title='Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f730d0-48b0-4cb7-91b9-6b9bc9812820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef131685-cf74-4c33-a3b4-e7056c107bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bffa51-f965-4304-a570-9484ec53fc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987103c-7522-48d4-98fe-6d9d57af57de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
