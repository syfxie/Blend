{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gram_matrix import gram_matrix\n",
    "import cv2 as cv\n",
    "# import matplotib as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 19:43:54.652480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574710816/574710816 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# the VGG-19 model is chosen over VGG-16 for better performance\n",
    "\n",
    "# call pretrained VGG19 model\n",
    "# def load_vgg_19():\n",
    "    # include_top includes the dense layers after the block5_pool layer\n",
    "    # model uses pretrained weights from imagenet\n",
    "vgg = keras.applications.VGG19(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 --> [(None, 224, 224, 3)]\n",
      "block1_conv1 --> (None, 224, 224, 64)\n",
      "block1_conv2 --> (None, 224, 224, 64)\n",
      "block1_pool --> (None, 112, 112, 64)\n",
      "block2_conv1 --> (None, 112, 112, 128)\n",
      "block2_conv2 --> (None, 112, 112, 128)\n",
      "block2_pool --> (None, 56, 56, 128)\n",
      "block3_conv1 --> (None, 56, 56, 256)\n",
      "block3_conv2 --> (None, 56, 56, 256)\n",
      "block3_conv3 --> (None, 56, 56, 256)\n",
      "block3_conv4 --> (None, 56, 56, 256)\n",
      "block3_pool --> (None, 28, 28, 256)\n",
      "block4_conv1 --> (None, 28, 28, 512)\n",
      "block4_conv2 --> (None, 28, 28, 512)\n",
      "block4_conv3 --> (None, 28, 28, 512)\n",
      "block4_conv4 --> (None, 28, 28, 512)\n",
      "block4_pool --> (None, 14, 14, 512)\n",
      "block5_conv1 --> (None, 14, 14, 512)\n",
      "block5_conv2 --> (None, 14, 14, 512)\n",
      "block5_conv3 --> (None, 14, 14, 512)\n",
      "block5_conv4 --> (None, 14, 14, 512)\n",
      "block5_pool --> (None, 7, 7, 512)\n",
      "flatten --> (None, 25088)\n",
      "fc1 --> (None, 4096)\n",
      "fc2 --> (None, 4096)\n",
      "predictions --> (None, 1000)\n"
     ]
    }
   ],
   "source": [
    "# print layers and output shape for each layer\n",
    "for layer in vgg.layers:\n",
    "    print(f'{layer.name} --> {layer.output_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content layers\n",
    "content_layers = ['block4_conv2']\n",
    "\n",
    "# style layers\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg_layers(layers):\n",
    "    vgg = keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    # weights are immutable\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(layer).output for layer in layers]\n",
    "\n",
    "    # build model\n",
    "    model = keras.Model([vgg.input], outputs)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "  # set max dimensions\n",
    "  max_dim = 512\n",
    "\n",
    "  # read image and convert to float\n",
    "  img = cv.imread(path)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  # scale image\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  # resize image\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n",
       "array([[[0.6313726 , 0.6627451 , 0.48235297],\n",
       "        [0.6156863 , 0.58431375, 0.33333334],\n",
       "        [0.5529412 , 0.4666667 , 0.16862746],\n",
       "        ...,\n",
       "        [0.6627451 , 0.45098042, 0.2627451 ],\n",
       "        [0.63529414, 0.44705886, 0.3254902 ],\n",
       "        [0.61960787, 0.54901963, 0.52156866]],\n",
       "\n",
       "       [[0.72156864, 0.6784314 , 0.4039216 ],\n",
       "        [0.6313726 , 0.5411765 , 0.21568629],\n",
       "        [0.6509804 , 0.52156866, 0.15686275],\n",
       "        ...,\n",
       "        [0.74509805, 0.46274513, 0.27450982],\n",
       "        [0.69411767, 0.43137258, 0.3019608 ],\n",
       "        [0.72156864, 0.6156863 , 0.56078434]],\n",
       "\n",
       "       [[0.6901961 , 0.5764706 , 0.21568629],\n",
       "        [0.6862745 , 0.54509807, 0.14901961],\n",
       "        [0.7137255 , 0.5568628 , 0.14509805],\n",
       "        ...,\n",
       "        [0.7490196 , 0.40784317, 0.21960786],\n",
       "        [0.6627451 , 0.34117648, 0.19215688],\n",
       "        [0.7490196 , 0.59607846, 0.48627454]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.41176474, 0.69411767, 0.75294125],\n",
       "        [0.3803922 , 0.75294125, 0.81568635],\n",
       "        [0.44705886, 0.8117648 , 0.89019614],\n",
       "        ...,\n",
       "        [0.7960785 , 0.75294125, 0.5921569 ],\n",
       "        [0.7686275 , 0.7411765 , 0.60784316],\n",
       "        [0.70980394, 0.76470596, 0.72156864]],\n",
       "\n",
       "       [[0.4666667 , 0.74509805, 0.81568635],\n",
       "        [0.43529415, 0.7843138 , 0.85098046],\n",
       "        [0.454902  , 0.7725491 , 0.8470589 ],\n",
       "        ...,\n",
       "        [0.7803922 , 0.7568628 , 0.6392157 ],\n",
       "        [0.7960785 , 0.8000001 , 0.7294118 ],\n",
       "        [0.7803922 , 0.8588236 , 0.8588236 ]],\n",
       "\n",
       "       [[0.454902  , 0.7294118 , 0.7960785 ],\n",
       "        [0.46274513, 0.80392164, 0.86274517],\n",
       "        [0.5411765 , 0.8235295 , 0.89019614],\n",
       "        ...,\n",
       "        [0.82745105, 0.8196079 , 0.73333335],\n",
       "        [0.8117648 , 0.83921576, 0.80392164],\n",
       "        [0.7490196 , 0.8352942 , 0.8588236 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load content image as 256 by 256 image\n",
    "content_img = cv.resize(src=cv.imread('content.jpg'), dsize=(256, 256))\n",
    "content_img.shape\n",
    "# represent images using float32 since VGG model expects float32 inputs\n",
    "content_img = tf.image.convert_image_dtype(content_img, dtype=tf.float32)\n",
    "content_img\n",
    "\n",
    "# load style image\n",
    "style_img = cv.resize(src=cv.imread('style.jpg'), dsize=(256, 256))\n",
    "# convert style image into float32\n",
    "style_img = tf.image.convert_image_dtype(style_img, dtype=tf.float32)\n",
    "style_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load content and style images\n",
    "content_img = load_img('content.jpg')\n",
    "style_img = load_img('style.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1\n",
      "shape:  (1, 361, 512, 64)\n",
      "mean:  43.533783\n",
      "block2_conv1\n",
      "shape:  (1, 180, 256, 128)\n",
      "mean:  171.75464\n",
      "block3_conv1\n",
      "shape:  (1, 90, 128, 256)\n",
      "mean:  139.94604\n",
      "block4_conv1\n",
      "shape:  (1, 45, 64, 512)\n",
      "mean:  587.8751\n",
      "block5_conv1\n",
      "shape:  (1, 22, 32, 512)\n",
      "mean:  36.141705\n"
     ]
    }
   ],
   "source": [
    "# extract style layers\n",
    "get_img_style = load_vgg_layers(style_layers)\n",
    "\n",
    "# style outputs\n",
    "style_outputs = get_img_style(style_img * 255)\n",
    "\n",
    "# examine each layer output\n",
    "for layer, output_ in zip(style_layers, style_outputs):\n",
    "    print(layer)\n",
    "    print('shape: ', np.shape(output_))\n",
    "    print('mean: ', np.mean(output_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# display graphs inside notebook\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mmatplotlib\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39minline\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mimshow(cv\u001b[39m.\u001b[39mcvtColor(np\u001b[39m.\u001b[39marray(content_img), cv\u001b[39m.\u001b[39mCOLOR_BGR2RGB))\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2414\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2413\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2414\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2416\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2417\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/IPython/core/magics/pylab.py:99\u001b[0m, in \u001b[0;36mPylabMagics.matplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAvailable matplotlib backends: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m backends_list)\n\u001b[1;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     gui, backend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshell\u001b[39m.\u001b[39;49menable_matplotlib(args\u001b[39m.\u001b[39;49mgui\u001b[39m.\u001b[39;49mlower() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(args\u001b[39m.\u001b[39;49mgui, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m args\u001b[39m.\u001b[39;49mgui)\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_show_matplotlib_backend(args\u001b[39m.\u001b[39mgui, backend)\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585\u001b[0m, in \u001b[0;36mInteractiveShell.enable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3564\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menable_matplotlib\u001b[39m(\u001b[39mself\u001b[39m, gui\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   3565\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Enable interactive matplotlib and inline figure support.\u001b[39;00m\n\u001b[1;32m   3566\u001b[0m \n\u001b[1;32m   3567\u001b[0m \u001b[39m    This takes the following steps:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[39m        display figures inline.\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3585\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib_inline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_inline\u001b[39;00m \u001b[39mimport\u001b[39;00m configure_inline_support\n\u001b[1;32m   3587\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m pylabtools \u001b[39mas\u001b[39;00m pt\n\u001b[1;32m   3588\u001b[0m     gui, backend \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mfind_gui_and_backend(gui, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpylab_gui_select)\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/matplotlib_inline/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m backend_inline, config  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.1.6\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/matplotlib_inline/backend_inline.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"A matplotlib backend for publishing figures via display_data\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Copyright (c) IPython Development Team.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Distributed under the terms of the BSD 3-Clause License.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m colors\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m \u001b[39mimport\u001b[39;00m backend_agg\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# display graphs inside notebook\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv.cvtColor(np.array(content_img), cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv.cvtColor(np.array(style_img), cv.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "# beta_1 = 0.9 means that we average over the last 10 iterations' gradients\n",
    "# higher beta_1 = averaging over more iterations\n",
    "opt = tf.optimizers.Adam(learning_rate = 0.01, epsilon=0.1, beta_1=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total loss = weighted average of content and style loss\n",
    "def total_loss(content_output, style_output, content_target, style_target):\n",
    "    # content weight\n",
    "    content_weight = 0.001\n",
    "\n",
    "    # style weight\n",
    "    style_weight = 0.001\n",
    "\n",
    "    # content loss\n",
    "    content_loss = tf.reduce_mean((content_output - content_target) ** 2)\n",
    "\n",
    "    # style loss\n",
    "    # match each style output with corresponding target\n",
    "    style_loss = tf.add_n([tf.reduce_mean(((output_ - target_) ** 2)) for output_, target_ in zip(style_output, style_target)])\n",
    "\n",
    "    # combine losses\n",
    "    total_loss = content_weight * content_loss + style_weight + style_loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating gram matrix: \n",
      "original shape: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, None, None, 64], name='tf.compat.v1.shape_50/Shape:0', description=\"created by layer 'tf.compat.v1.shape_50'\")\n",
      "flattened tensor: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, None], name='tf.compat.v1.shape_51/Shape:0', description=\"created by layer 'tf.compat.v1.shape_51'\")\n",
      "gram matrix: \n",
      "calculating gram matrix: \n",
      "original shape: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, None, None, 128], name='tf.compat.v1.shape_52/Shape:0', description=\"created by layer 'tf.compat.v1.shape_52'\")\n",
      "flattened tensor: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, None], name='tf.compat.v1.shape_53/Shape:0', description=\"created by layer 'tf.compat.v1.shape_53'\")\n",
      "gram matrix: \n",
      "calculating gram matrix: \n",
      "original shape: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, None, None, 256], name='tf.compat.v1.shape_54/Shape:0', description=\"created by layer 'tf.compat.v1.shape_54'\")\n",
      "flattened tensor: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, None], name='tf.compat.v1.shape_55/Shape:0', description=\"created by layer 'tf.compat.v1.shape_55'\")\n",
      "gram matrix: \n",
      "calculating gram matrix: \n",
      "original shape: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, None, None, 512], name='tf.compat.v1.shape_56/Shape:0', description=\"created by layer 'tf.compat.v1.shape_56'\")\n",
      "flattened tensor: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, None], name='tf.compat.v1.shape_57/Shape:0', description=\"created by layer 'tf.compat.v1.shape_57'\")\n",
      "gram matrix: \n",
      "calculating gram matrix: \n",
      "original shape: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, None, None, 512], name='tf.compat.v1.shape_58/Shape:0', description=\"created by layer 'tf.compat.v1.shape_58'\")\n",
      "flattened tensor: \n",
      "KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None), inferred_value=[None, None], name='tf.compat.v1.shape_59/Shape:0', description=\"created by layer 'tf.compat.v1.shape_59'\")\n",
      "gram matrix: \n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'tf.reshape_29' (type TFOpLambda).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 131072 values, but the requested shape has 256 [Op:Reshape]\n\nCall arguments received by layer 'tf.reshape_29' (type TFOpLambda):\n  • tensor=tf.Tensor(shape=(1, 16, 16, 512), dtype=float32)\n  • shape=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vgg_model \u001b[39m=\u001b[39m load_vgg()\n\u001b[0;32m----> 2\u001b[0m content_target \u001b[39m=\u001b[39m vgg_model(np\u001b[39m.\u001b[39;49marray([content_img \u001b[39m*\u001b[39;49m \u001b[39m255\u001b[39;49m]))[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m style_target \u001b[39m=\u001b[39m vgg_model(np\u001b[39m.\u001b[39maray([style_img \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m]))[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/anaconda3/envs/jupyterlab-ext/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'tf.reshape_29' (type TFOpLambda).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 131072 values, but the requested shape has 256 [Op:Reshape]\n\nCall arguments received by layer 'tf.reshape_29' (type TFOpLambda):\n  • tensor=tf.Tensor(shape=(1, 16, 16, 512), dtype=float32)\n  • shape=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']\n  • name=None"
     ]
    }
   ],
   "source": [
    "vgg_model = load_vgg()\n",
    "content_target = vgg_model(np.array([content_img * 255]))[0]\n",
    "style_target = vgg_model(np.aray([style_img * 255]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(image, epoch):\n",
    "    with tf.GradientTape as tape:\n",
    "        # pass image to model\n",
    "        output = vgg_model(image * 255)\n",
    "        # calculate total loss\n",
    "        loss = total_loss(output[0], output[1], content_target, style_target)\n",
    "        gradient = tape.grandient(loss, image)\n",
    "        # optimize\n",
    "        opt.apply_gradients([(gradient, image)])\n",
    "        image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n",
    "\n",
    "        # check loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            tf.print(f'Loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m content \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable([content])\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 6\u001b[0m     train_model(content, i)\n",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(image, epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(image, epoch):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m      3\u001b[0m         \u001b[39m# pass image to model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         output \u001b[39m=\u001b[39m vgg_model(image \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\n\u001b[1;32m      5\u001b[0m         \u001b[39m# calculate total loss\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "content = tf.image.convert_image_dtype(content_img, tf.float32)\n",
    "content = tf.Variable([content])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train_model(content, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
