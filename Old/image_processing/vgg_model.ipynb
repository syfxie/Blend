{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gram_matrix import gram_matrix\n",
    "import cv2 as cv\n",
    "# import matplotib as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the VGG-19 model is chosen over VGG-16 for better performance\n",
    "\n",
    "# call pretrained VGG19 model\n",
    "# def load_vgg_19():\n",
    "    # include_top includes the dense layers after the block5_pool layer\n",
    "    # model uses pretrained weights from imagenet\n",
    "vgg = keras.applications.VGG19(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print layers and output shape for each layer\n",
    "for layer in vgg.layers:\n",
    "    print(f'{layer.name} --> {layer.output_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content layers\n",
    "content_layers = ['block4_conv2']\n",
    "\n",
    "# style layers\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg_layers(layers):\n",
    "    vgg = keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    # weights are immutable\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(layer).output for layer in layers]\n",
    "\n",
    "    # build model\n",
    "    model = keras.Model([vgg.input], outputs)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "  # set max dimensions\n",
    "  max_dim = 512\n",
    "\n",
    "  # read image and convert to float\n",
    "  img = tf.io.read_file(path)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  # scale image\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  # resize image\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load content image as 256 by 256 image\n",
    "content_img = cv.resize(src=cv.imread('content.jpg'), dsize=(256, 256))\n",
    "content_img.shape\n",
    "# represent images using float32 since VGG model expects float32 inputs\n",
    "content_img = tf.image.convert_image_dtype(content_img, dtype=tf.float32)\n",
    "content_img\n",
    "\n",
    "# load style image\n",
    "style_img = cv.resize(src=cv.imread('style.jpg'), dsize=(256, 256))\n",
    "# convert style image into float32\n",
    "style_img = tf.image.convert_image_dtype(style_img, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load content and style images\n",
    "\n",
    "content_img = load_img('/Users/sophiexie/CS Projects/PicYourStyle/res/content.jpg')\n",
    "style_img = load_img('/Users/sophiexie/CS Projects/PicYourStyle/res/style.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1\n",
      "shape:  (1, 361, 512, 64)\n",
      "mean:  43.533783\n",
      "block2_conv1\n",
      "shape:  (1, 180, 256, 128)\n",
      "mean:  171.75464\n",
      "block3_conv1\n",
      "shape:  (1, 90, 128, 256)\n",
      "mean:  139.94604\n",
      "block4_conv1\n",
      "shape:  (1, 45, 64, 512)\n",
      "mean:  587.8751\n",
      "block5_conv1\n",
      "shape:  (1, 22, 32, 512)\n",
      "mean:  36.141705\n"
     ]
    }
   ],
   "source": [
    "# extract style layers\n",
    "get_img_style = load_vgg_layers(style_layers)\n",
    "\n",
    "# style outputs\n",
    "style_outputs = get_img_style(style_img * 255)\n",
    "\n",
    "# examine each layer output\n",
    "for layer, output_ in zip(style_layers, style_outputs):\n",
    "    print(layer)\n",
    "    print('shape: ', np.shape(output_))\n",
    "    print('mean: ', np.mean(output_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display graphs inside notebook\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv.cvtColor(np.array(content_img), cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv.cvtColor(np.array(style_img), cv.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "# beta_1 = 0.9 means that we average over the last 10 iterations' gradients\n",
    "# higher beta_1 = averaging over more iterations\n",
    "opt = tf.optimizers.Adam(learning_rate = 0.01, epsilon=0.1, beta_1=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total loss = weighted average of content and style loss\n",
    "def total_loss(content_output, style_output, content_target, style_target):\n",
    "    # content weight\n",
    "    content_weight = 0.001\n",
    "\n",
    "    # style weight\n",
    "    style_weight = 0.001\n",
    "\n",
    "    # content loss\n",
    "    content_loss = tf.reduce_mean((content_output - content_target) ** 2)\n",
    "\n",
    "    # style loss\n",
    "    # match each style output with corresponding target\n",
    "    style_loss = tf.add_n([tf.reduce_mean(((output_ - target_) ** 2)) for output_, target_ in zip(style_output, style_target)])\n",
    "\n",
    "    # combine losses\n",
    "    total_loss = content_weight * content_loss + style_weight + style_loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = load_vgg()\n",
    "content_target = vgg_model(np.array([content_img * 255]))[0]\n",
    "style_target = vgg_model(np.aray([style_img * 255]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(image, epoch):\n",
    "    with tf.GradientTape as tape:\n",
    "        # pass image to model\n",
    "        output = vgg_model(image * 255)\n",
    "        # calculate total loss\n",
    "        loss = total_loss(output[0], output[1], content_target, style_target)\n",
    "        gradient = tape.grandient(loss, image)\n",
    "        # optimize\n",
    "        opt.apply_gradients([(gradient, image)])\n",
    "        image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n",
    "\n",
    "        # check loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            tf.print(f'Loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "content = tf.image.convert_image_dtype(content_img, tf.float32)\n",
    "content = tf.Variable([content])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train_model(content, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
